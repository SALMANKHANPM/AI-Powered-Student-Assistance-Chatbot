{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c8718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /home/salmankhanpm786/.local/lib/python3.12/site-packages (4.24.0)\n",
      "Requirement already satisfied: duckduckgo_search in /home/salmankhanpm786/.local/lib/python3.12/site-packages (6.2.11)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: webdriver_manager in /home/salmankhanpm786/.local/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in /home/salmankhanpm786/.local/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /usr/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.19)\n",
      "Requirement already satisfied: trio~=0.17 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: click>=8.1.7 in /usr/lib/python3.12/site-packages (from duckduckgo_search) (8.1.7)\n",
      "Requirement already satisfied: primp>=0.6.1 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from duckduckgo_search) (0.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.12/site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from webdriver_manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.12/site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.12/site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium duckduckgo_search beautifulsoup4  webdriver_manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e2f5e5-71c2-493b-b8ed-fe5234e4f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from duckduckgo_search import DDGS\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e43dd69-7a2d-4e7f-af97-f0a5e9dbc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(name, url):\n",
    "    # Configure Selenium options (headless browser, if desired)\n",
    "    options = Options()\n",
    "    options.headless = True  # Set to False if you want to see the browser interaction\n",
    "\n",
    "    # Specify the path to chromedriver.exe\n",
    "    # driver_path = 'chromedriver.exe'  # Update with your actual path to chromedriver\n",
    "\n",
    "    # # Initialize the WebDriver with Chrome\n",
    "    # service = Service(driver_path)\n",
    "    # driver = webdriver.Chrome(service=service, options=options)\n",
    "    options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to fully render (adjust wait time as needed)\n",
    "    time.sleep(5)  # Example: Wait for 5 seconds (you can use more sophisticated waits)\n",
    "\n",
    "    # Extract the page content after rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Close the WebDriver session\n",
    "    driver.quit()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce559d5-585d-41a5-ab50-e888451b152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove extra whitespace and newlines\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df7358c-9d19-43bf-a283-c749d641294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_element(element):\n",
    "    if element.name in ['p', 'ul', 'ol', 'table']:\n",
    "        return clean_text(element.get_text())\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47fc279-0550-405a-8492-b8b4a5aa0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(soup):\n",
    "\n",
    "    # Remove all script and style elements\n",
    "    for script in soup([\"script\", \"style\", \"img\", \"link\", \"href\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    parse_data = {\n",
    "        \"title\": \"\",\n",
    "        \"meta_description\": \"\",\n",
    "        \"keywords\": \"\",\n",
    "        \"basic_info\": {},\n",
    "    }\n",
    "\n",
    "    # Extract title\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        parse_data[\"title\"] = clean_text(title_tag.string)\n",
    "\n",
    "    # Extract meta description\n",
    "    meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "    if meta_desc:\n",
    "        parse_data[\"meta_description\"] = clean_text(meta_desc.get('content', ''))\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords:\n",
    "        parse_data[\"keywords\"] = clean_text(keywords.get('content', ''))\n",
    "\n",
    "    # Extract structured data\n",
    "    structured_data_tags = soup.find_all('script', type='application/ld+json')\n",
    "    for tag in structured_data_tags:\n",
    "        try:\n",
    "            data = json.loads(tag.string)\n",
    "            parse_data[\"structured_data\"].append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Ignore invalid JSON\n",
    "\n",
    "    # Extract basic information from the first section\n",
    "    parse_data[\"basic_info\"]=[]\n",
    "    parse_data[\"Section\"] = {}\n",
    "    about_section = soup.find('section', class_='bg-white rounded-16 p-6')\n",
    "    if about_section:\n",
    "        parse_data[\"Section\"][\"name\"] = clean_text(about_section.find('h2').text) if about_section.find('h2') else \"\"\n",
    "        content_div = about_section.find('div', class_='content-section')\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            parse_data[\"Section\"][\"description\"] = \"\\n\".join([clean_text(p.text) for p in paragraphs])\n",
    "            \n",
    "            # Find all headers (h2, h3, h4, etc.) in the content div\n",
    "            headers = content_div.find_all(['h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "            \n",
    "            for header in headers:\n",
    "                header_text = clean_text(header.text)\n",
    "                description = []\n",
    "                \n",
    "                # Collect all paragraph siblings until the next header or end of content\n",
    "                for sibling in header.find_next_siblings():\n",
    "                    if sibling.name in ['h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        break\n",
    "                    if sibling.name == 'p':\n",
    "                        description.append(extract_text_from_element(sibling))\n",
    "                \n",
    "                parse_data[\"basic_info\"].append({\n",
    "                    \"name\": header_text,\n",
    "                    \"description\": [item for item in description if item]\n",
    "                })\n",
    "                \n",
    "    \n",
    "    # extract tabular data\n",
    "    tables = soup.find_all('table', class_=lambda x: x is not None and x.strip() != \"\")\n",
    "    for table in tables:\n",
    "        header = table.find_previous('h2')\n",
    "        header_text = header.text.strip() if header else \"No header found\"\n",
    "\n",
    "        # Extract headers\n",
    "        headers = [th.text.strip() for th in table.find_all('th')]\n",
    "\n",
    "        # Initialize the result list\n",
    "        table_data = []\n",
    "\n",
    "        # Track rowspans\n",
    "        rowspan_tracker = [0] * len(headers)\n",
    "\n",
    "        for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            row_data = {}\n",
    "            col_index = 0\n",
    "\n",
    "            for cell in cells:\n",
    "                while col_index < len(headers) and rowspan_tracker[col_index] > 0:\n",
    "                    if table_data:\n",
    "                        row_data[headers[col_index]] = table_data[-1].get(headers[col_index], \"\")\n",
    "                    rowspan_tracker[col_index] -= 1\n",
    "                    col_index += 1\n",
    "\n",
    "                if col_index < len(headers):\n",
    "                    rowspan = int(cell.get('rowspan', 1))\n",
    "                    colspan = int(cell.get('colspan', 1))\n",
    "\n",
    "                    for _ in range(colspan):\n",
    "                        if col_index < len(headers):\n",
    "                            row_data[headers[col_index]] = cell.text.strip()\n",
    "                            if rowspan > 1:\n",
    "                                rowspan_tracker[col_index] = rowspan - 1\n",
    "                            col_index += 1\n",
    "\n",
    "            if row_data:  # Avoid adding empty rows\n",
    "                table_data.append(row_data)\n",
    "\n",
    "        parse_data[header_text] = table_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Initialize an empty list to store the FAQs\n",
    "    faqs = []\n",
    "    # Find all the question and answer pairs\n",
    "    faq_div = soup.find('div', class_='cdcms_faqs')\n",
    "    questions = faq_div.find_all('p', class_='accordio')\n",
    "    answers = faq_div.find_all('div', class_='liv')\n",
    "\n",
    "    # Iterate through the questions and answers and store them in a structured format\n",
    "    for question, answer in zip(questions, answers):\n",
    "        faq_item = {\n",
    "            'question': question.get_text(strip=True).replace(\"Ques. \", \"\"),\n",
    "            'answer': answer.get_text(strip=True).replace(\"Ans. \", \"\")\n",
    "        }\n",
    "        faqs.append(faq_item)\n",
    "    parse_data['faqs']=faqs\n",
    "    return parse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707507d-1780-400e-8b68-94663e8428d6",
   "metadata": {},
   "source": [
    "name = 'SRM'\n",
    "url = 'https://collegedunia.com/university/25896-srm-institute-of-science-and-technology-srmist-chennai'\n",
    "\n",
    "extracted = extract(name, url)\n",
    "Scraper(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f63ee5-cb06-47da-88d9-523a0ac174d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_college_on_duckduckgo(college_name):\n",
    "    query = f\"{college_name} site:collegedunia.com\"\n",
    "    \n",
    "    # Create a DDGS object and perform a search\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.text(query, max_results=1)\n",
    "    res=[]    \n",
    "    if results:\n",
    "        for result in results:\n",
    "            res.append(result['href'])\n",
    "        return res\n",
    "    else:\n",
    "        return \"No CollegeDunia link found in the search results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176e74d0-e20e-48de-895e-4c3dda731fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollege_names.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 6\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m web \u001b[38;5;129;01min\u001b[39;00m search_college_on_duckduckgo(name):\n\u001b[1;32m      8\u001b[0m         extracted \u001b[38;5;241m=\u001b[39m extract(name, web)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv(\"college_names.csv\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    name = row['College Name']\n",
    "    for web in search_college_on_duckduckgo(name):\n",
    "        extracted = extract(name, web)\n",
    "        with open(f\"{name}.json\", 'w', encoding='utf-8') as js:\n",
    "            json.dump(scrape(extracted), js, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cff82-6322-4876-b5d2-d358c15efea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

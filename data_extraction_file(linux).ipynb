{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa557f57-9d68-4a3f-9d3f-a14639250212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /home/salmankhanpm786/.local/lib/python3.12/site-packages (4.24.0)\n",
      "Requirement already satisfied: duckduckgo_search in /home/salmankhanpm786/.local/lib/python3.12/site-packages (6.2.11)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: webdriver_manager in /home/salmankhanpm786/.local/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in /home/salmankhanpm786/.local/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /usr/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.19)\n",
      "Requirement already satisfied: trio~=0.17 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: click>=8.1.7 in /usr/lib/python3.12/site-packages (from duckduckgo_search) (8.1.7)\n",
      "Requirement already satisfied: primp>=0.6.1 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from duckduckgo_search) (0.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.12/site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from webdriver_manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.12/site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.12/site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/salmankhanpm786/.local/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium duckduckgo_search beautifulsoup4  webdriver_manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3e2f5e5-71c2-493b-b8ed-fe5234e4f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from duckduckgo_search import DDGS\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e43dd69-7a2d-4e7f-af97-f0a5e9dbc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(name, url):\n",
    "    # Configure Selenium options (headless browser, if desired)\n",
    "    options = Options()\n",
    "    options.headless = True  # Set to False if you want to see the browser interaction\n",
    "\n",
    "    # Specify the path to chromedriver.exe\n",
    "    # driver_path = 'chromedriver.exe'  # Update with your actual path to chromedriver\n",
    "\n",
    "    # # Initialize the WebDriver with Chrome\n",
    "    # service = Service(driver_path)\n",
    "    # driver = webdriver.Chrome(service=service, options=options)\n",
    "    options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to fully render (adjust wait time as needed)\n",
    "    time.sleep(5)  # Example: Wait for 5 seconds (you can use more sophisticated waits)\n",
    "\n",
    "    # Extract the page content after rendering\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Close the WebDriver session\n",
    "    driver.quit()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce559d5-585d-41a5-ab50-e888451b152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove extra whitespace and newlines\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1df7358c-9d19-43bf-a283-c749d641294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_element(element):\n",
    "    if element.name in ['p', 'ul', 'ol', 'table']:\n",
    "        return clean_text(element.get_text())\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f47fc279-0550-405a-8492-b8b4a5aa0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(soup):\n",
    "\n",
    "    # Remove all script and style elements\n",
    "    for script in soup([\"script\", \"style\", \"img\", \"link\", \"href\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    parse_data = {\n",
    "        \"title\": \"\",\n",
    "        \"meta_description\": \"\",\n",
    "        \"keywords\": \"\",\n",
    "        \"basic_info\": {},\n",
    "    }\n",
    "\n",
    "    # Extract title\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        parse_data[\"title\"] = clean_text(title_tag.string)\n",
    "\n",
    "    # Extract meta description\n",
    "    meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "    if meta_desc:\n",
    "        parse_data[\"meta_description\"] = clean_text(meta_desc.get('content', ''))\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords:\n",
    "        parse_data[\"keywords\"] = clean_text(keywords.get('content', ''))\n",
    "\n",
    "    # Extract structured data\n",
    "    structured_data_tags = soup.find_all('script', type='application/ld+json')\n",
    "    for tag in structured_data_tags:\n",
    "        try:\n",
    "            data = json.loads(tag.string)\n",
    "            parse_data[\"structured_data\"].append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Ignore invalid JSON\n",
    "\n",
    "    # Extract basic information from the first section\n",
    "    parse_data[\"basic_info\"]=[]\n",
    "    parse_data[\"Section\"] = {}\n",
    "    about_section = soup.find('section', class_='bg-white rounded-16 p-6')\n",
    "    if about_section:\n",
    "        parse_data[\"Section\"][\"name\"] = clean_text(about_section.find('h2').text) if about_section.find('h2') else \"\"\n",
    "        content_div = about_section.find('div', class_='content-section')\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            parse_data[\"Section\"][\"description\"] = \"\\n\".join([clean_text(p.text) for p in paragraphs])\n",
    "            \n",
    "            # Find all headers (h2, h3, h4, etc.) in the content div\n",
    "            headers = content_div.find_all(['h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "            \n",
    "            for header in headers:\n",
    "                header_text = clean_text(header.text)\n",
    "                description = []\n",
    "                \n",
    "                # Collect all paragraph siblings until the next header or end of content\n",
    "                for sibling in header.find_next_siblings():\n",
    "                    if sibling.name in ['h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        break\n",
    "                    if sibling.name == 'p':\n",
    "                        description.append(extract_text_from_element(sibling))\n",
    "                \n",
    "                parse_data[\"basic_info\"].append({\n",
    "                    \"name\": header_text,\n",
    "                    \"description\": [item for item in description if item]\n",
    "                })\n",
    "                \n",
    "    \n",
    "    # extract tabular data\n",
    "    tables = soup.find_all('table', class_=lambda x: x is not None and x.strip() != \"\")\n",
    "    for table in tables:\n",
    "        header = table.find_previous('h2')\n",
    "        header_text = header.text.strip() if header else \"No header found\"\n",
    "\n",
    "        # Extract headers\n",
    "        headers = [th.text.strip() for th in table.find_all('th')]\n",
    "\n",
    "        # Initialize the result list\n",
    "        table_data = []\n",
    "\n",
    "        # Track rowspans\n",
    "        rowspan_tracker = [0] * len(headers)\n",
    "\n",
    "        for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            row_data = {}\n",
    "            col_index = 0\n",
    "\n",
    "            for cell in cells:\n",
    "                while col_index < len(headers) and rowspan_tracker[col_index] > 0:\n",
    "                    if table_data:\n",
    "                        row_data[headers[col_index]] = table_data[-1].get(headers[col_index], \"\")\n",
    "                    rowspan_tracker[col_index] -= 1\n",
    "                    col_index += 1\n",
    "\n",
    "                if col_index < len(headers):\n",
    "                    rowspan = int(cell.get('rowspan', 1))\n",
    "                    colspan = int(cell.get('colspan', 1))\n",
    "\n",
    "                    for _ in range(colspan):\n",
    "                        if col_index < len(headers):\n",
    "                            row_data[headers[col_index]] = cell.text.strip()\n",
    "                            if rowspan > 1:\n",
    "                                rowspan_tracker[col_index] = rowspan - 1\n",
    "                            col_index += 1\n",
    "\n",
    "            if row_data:  # Avoid adding empty rows\n",
    "                table_data.append(row_data)\n",
    "\n",
    "        parse_data[header_text] = table_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Initialize an empty list to store the FAQs\n",
    "    faqs = []\n",
    "    # Find all the question and answer pairs\n",
    "    faq_div = soup.find('div', class_='cdcms_faqs')\n",
    "    if faq_div:\n",
    "        questions = faq_div.find_all('p', class_='accordio')\n",
    "        answers = faq_div.find_all('div', class_='liv')\n",
    "\n",
    "        # Iterate through the questions and answers and store them in a structured format\n",
    "        for question, answer in zip(questions, answers):\n",
    "            faq_item = {\n",
    "                'question': question.get_text(strip=True).replace(\"Ques. \", \"\"),\n",
    "                'answer': answer.get_text(strip=True).replace(\"Ans. \", \"\")\n",
    "            }\n",
    "            faqs.append(faq_item)\n",
    "    parse_data['faqs']=faqs\n",
    "    return parse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707507d-1780-400e-8b68-94663e8428d6",
   "metadata": {},
   "source": [
    "name = 'SRM'\n",
    "url = 'https://collegedunia.com/university/25896-srm-institute-of-science-and-technology-srmist-chennai'\n",
    "\n",
    "extracted = extract(name, url)\n",
    "Scraper(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19f63ee5-cb06-47da-88d9-523a0ac174d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_college_on_duckduckgo(college_name):\n",
    "    query = f\"{college_name} site:collegedunia.com\"\n",
    "    \n",
    "    # Create a DDGS object and perform a search\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.text(query, max_results=1)\n",
    "    res=[]    \n",
    "    if results:\n",
    "        for result in results:\n",
    "            res.append(result['href'])\n",
    "        return res\n",
    "    else:\n",
    "        return \"No CollegeDunia link found in the search results.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4860034-5dca-42b8-8218-bef35d9acf30",
   "metadata": {},
   "source": [
    "name = 'college_names6.csv'\n",
    "df = pd.read_csv(name)\n",
    "df['College Name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "176e74d0-e20e-48de-895e-4c3dda731fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RatelimitException",
     "evalue": "https://duckduckgo.com/ 202 Ratelimit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRatelimitException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m76\u001b[39m, \u001b[38;5;241m88\u001b[39m):\n\u001b[1;32m      8\u001b[0m     name \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollege Name\u001b[39m\u001b[38;5;124m'\u001b[39m][l]\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m web \u001b[38;5;129;01min\u001b[39;00m \u001b[43msearch_college_on_duckduckgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m web:\n\u001b[1;32m     11\u001b[0m             extracted \u001b[38;5;241m=\u001b[39m extract(name, web)\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36msearch_college_on_duckduckgo\u001b[0;34m(college_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a DDGS object and perform a search\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DDGS() \u001b[38;5;28;01mas\u001b[39;00m ddgs:\n\u001b[0;32m----> 6\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mddgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m res\u001b[38;5;241m=\u001b[39m[]    \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/duckduckgo_search/duckduckgo_search.py:239\u001b[0m, in \u001b[0;36mDDGS.text\u001b[0;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[1;32m    236\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml is not installed. Using backend=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafesearch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimelimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    241\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_html(keywords, region, timelimit, max_results)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/duckduckgo_search/duckduckgo_search.py:273\u001b[0m, in \u001b[0;36mDDGS._text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"DuckDuckGo text search. Query params: https://duckduckgo.com/params.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    TimeoutException: Inherits from DuckDuckGoSearchException, raised for API request timeouts.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m keywords, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords is mandatory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 273\u001b[0m vqd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_vqd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: keywords,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl\u001b[39m\u001b[38;5;124m\"\u001b[39m: region,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    285\u001b[0m }\n\u001b[1;32m    286\u001b[0m safesearch \u001b[38;5;241m=\u001b[39m safesearch\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/duckduckgo_search/duckduckgo_search.py:134\u001b[0m, in \u001b[0;36mDDGS._get_vqd\u001b[0;34m(self, keywords)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_vqd\u001b[39m(\u001b[38;5;28mself\u001b[39m, keywords: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get vqd value for a search query.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     resp_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://duckduckgo.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _extract_vqd(resp_content, keywords)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/duckduckgo_search/duckduckgo_search.py:129\u001b[0m, in \u001b[0;36mDDGS._get_url\u001b[0;34m(self, method, url, params, content, data)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_event\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m202\u001b[39m, \u001b[38;5;241m301\u001b[39m, \u001b[38;5;241m403\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RatelimitException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Ratelimit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m return None. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRatelimitException\u001b[0m: https://duckduckgo.com/ 202 Ratelimit"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "name = 'college_names.csv'\n",
    "df = pd.read_csv(name)\n",
    "df['College Name'][0]\n",
    "\n",
    "\n",
    "for l in range(76, 88):\n",
    "    name = df['College Name'][l]\n",
    "    for web in search_college_on_duckduckgo(name):\n",
    "        if web:\n",
    "            extracted = extract(name, web)\n",
    "        else:\n",
    "            print(name)\n",
    "        with open(f\"{name}.json\",'w',encoding='utf-8') as js:\n",
    "            json.dump(scrape(extracted), js, ensure_ascii=False, indent=4)\n",
    "            l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cff82-6322-4876-b5d2-d358c15efea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
